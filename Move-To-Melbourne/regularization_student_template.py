# -*- coding: utf-8 -*-
"""regularization_student_template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TjYpri410stDvEiYFKMMF7BZyOHSUfkV
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

import warnings
warnings.filterwarnings('ignore')

from sklearn import datasets

p = ['a','b','c']
for i,j in enumerate(p):
    print(i, j)
p

"""### Load the dataset

- Load the train data and using all your knowledge of pandas try to explore the different statistical properties like correlation of the dataset.
"""

# Code starts here

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

reg_dict = pd.read_excel('RegularizationDataDictionary.xlsx')
samp_sub = pd.read_csv('sample_submission.csv')

Id = test['Id']


# Code ends here.

train.head()

train.Landsize.describe()

#reg_dict.set_index('Feature',inplace = True)
reg_dict.loc['Regionname'][0]

['Method','SellerG','Postcode','CouncilArea','Longitude']

train.drop('Id',axis = 1, inplace = True)
test.drop('Id', axis = 1, inplace = True)

train.Rooms.value_counts()
plt.figure(figsize = [12,10])
plt.title('Grouping houses by number of rooms', fontsize=18, fontweight='bold', y=1.05,)
plt.xlabel('Number of players', fontsize=12)
plt.ylabel('Players Age', fontsize=12)
sns.set_style('whitegrid')
sns.countplot(train.Rooms)

train.corr()

plt.figure(figsize = [12,10])
sns.heatmap(train.corr())

"""## Model building

- Separate the features and target and then split the train data into train and validation set.
- Apply different models of your choice and  then predict on the validation data and find the `accuracy_score` for this prediction.
- Try improving upon the `accuracy_score` using different regularization techniques.
"""

# Linear Regression

X = train.drop(['Price'], axis = 1)
y = train['Price']

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 4, test_size = 0.2)

linreg = LinearRegression()
linreg.fit(X_train, y_train)

pred = linreg.predict(X_test)

r2_score(pred, y_test)
# Code ends here.

# polynomial fit 

poly = PolynomialFeatures(2)

X_train_2 = poly.fit_transform(X_train)
X_test_2 = poly.fit_transform(X_test)

model = LinearRegression()

model.fit(X_train_2, y_train)

pred_2 = model.predict(X_test_2)

r2_score(pred_2, y_test)

# Regularization Lasso

lasso = Lasso()
lasso.fit(X_train_2, y_train)
lasso_pred = lasso.predict(X_test_2)

r2_score(lasso_pred, y_test)

# Regularization Ridge

ridge = Ridge()
ridge.fit(X_train_2, y_train)
ridge_pred = ridge.predict(X_test_2)

r2_score(ridge_pred, y_test)

"""### Prediction on the test data and creating the sample submission file.

- Load the test data and store the `Id` column in a separate variable.
- Perform the same operations on the test data that you have performed on the train data.
- Create the submission file as a `csv` file consisting of the `Id` column from the test data and your prediction as the second column.
"""

# Code starts here


#ridge_test_predict = ridge.predict(test)

#Id = test['Id']
#test.drop('Id', axis = 1, inplace = True)

test_2 = poly.fit_transform(test)

ridge_test_predict = ridge.predict(test_2)

print(ridge_test_predict)
# Code ends here.

test.corr()



# Using Cross Validation 
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
scorer = make_scorer(mean_squared_error, greater_is_better = False)

L1 = Lasso()
L2 = Ridge()


# cross validation with L1
rmse_L1 = -np.mean(cross_val_score(L1,X_train,y_train,scoring=scorer, cv=10))
print(rmse_L1)
# cross validation with L2
rmse_L2 = -np.mean(cross_val_score(L2,X_train,y_train,scoring=scorer, cv=10))
print(rmse_L2)

# Hyper-parameter Tuning

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import warnings
warnings.filterwarnings('ignore')

# regularization parameters for grid search
ridge_lambdas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]
lasso_lambdas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1]

# Code starts here

# instantiate lasso and ridge models
lasso_model = Lasso()
ridge_model = Ridge()

# grid search on lasso and ridge
ridge_grid = GridSearchCV(estimator = ridge_model, param_grid = dict(alpha = ridge_lambdas))
ridge_grid.fit(X_train_2,y_train)

lasso_grid = GridSearchCV(estimator = lasso_model, param_grid = dict(alpha = lasso_lambdas))
lasso_grid.fit(X_train_2,y_train)

# make predictions 
lasso_pred = lasso_grid.predict(X_test_2)
ridge_pred = ridge_grid.predict(X_test_2)

# print out which is better
lasso_rmse = np.sqrt(mean_squared_error(lasso_pred,y_test))
print(lasso_rmse)
ridge_rmse = np.sqrt(mean_squared_error(ridge_pred,y_test))
print(ridge_rmse)

if lasso_rmse < ridge_rmse:
    best_model = 'LASSO'
else:
    best_model = 'RIDGE'

print(best_model)
# Code ends here

r2_score(ridge_pred,y_test)

r2_score(lasso_pred,y_test)

ridge_predict = ridge_grid.predict(test_2)
lasso_predict = lasso_grid.predict(test_2)

submission = pd.DataFrame({'Id': Id, 'Price':ridge_predict})
submission.to_csv('third_submission.csv', index = False)

pd.DataFrame({'Id':Id, 'Price': lasso_predict})

"""## Using Pipeline"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

X = train.drop(['Price'], axis = 1)
y = train['Price']

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 349, test_size = 0.2)

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

pca = PCA(n_components = 5)
X = pca.fit_transform(X)

"""# Questions

1. Why is Standard Scaler giving a worse score for linear pregression?
2. Why does the r2_score change drastically with changing random_state input?
"""

linreg = LinearRegression()
linreg.fit(X_train, y_train)
preds = linreg.predict(X_test)
r2_score(preds, y_test)

poly = PolynomialFeatures(2)

X_train_2 = poly.fit_transform(X_train)
X_test_2 = poly.fit_transform(X_test)

test_2 = poly.fit_transform(test)

model = LinearRegression()

model.fit(X_train_2, y_train)

pred_2 = model.predict(X_test_2)

prediction = model.predict(test_2)
r2_score(pred_2, y_test)

X_test_2.shape

lasso = Lasso()
lasso.fit(X_train_2, y_train)
lasso_pred = lasso.predict(X_test_2)

lasso_prediction = lasso.predict(test_2)
r2_score(lasso_pred, y_test)

ridge = Ridge()
ridge.fit(X_train_2, y_train)
ridge_pred = ridge.predict(X_test_2)

ridge_prediction = ridge.predict(test_2)

r2_score(ridge_pred, y_test)

submission = pd.DataFrame({'Id': Id, 'Price':prediction})
submission.to_csv('forth_submission.csv', index = False)

submission

def Rscore_random_state(random_state):
    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = random_state, test_size = 0.2)
    
    linreg = LinearRegression()
    linreg.fit(X_train, y_train)
    preds = linreg.predict(X_test)
    a = r2_score(preds, y_test)
    
    poly = PolynomialFeatures(2)
    X_train_2 = poly.fit_transform(X_train)
    X_test_2 = poly.fit_transform(X_test)
    model = LinearRegression()
    model.fit(X_train_2, y_train)
    pred_2 = model.predict(X_test_2)
    b = r2_score(pred_2, y_test)
    
    #print('R2_score with Linear Regression for Random State {} is {}'.format(random_state, a))
    #print('R2_score with Polynomial Regression for Random State {} is {}'.format(random_state, b))
    
    return (random_state, a,b)

import matplotlib.pyplot as plt

lst_lr = []
lst_pr = []
lst_rs = []
plt.figure(figsize = [12,10])
plt.xlabel('Random_State')
plt.ylabel('R2 Score')
start = time.time()
for n in range(0,1000):
    random_state, a, b = Rscore_random_state(n)
    plt.scatter(random_state,a, color = 'darkblue', alpha = 0.5)
    plt.scatter(random_state,b, color = 'yellow', alpha = 0.5)
    #print(Rscore_random_state(n))
    lst_pr.append(b)
    lst_lr.append(a)
    lst_rs.append(random_state)
plt.legend(['Linear Regression', 'Polynomial Regression'])
end = time.time()
print('Duration: {}'.format(end-start))

import time
time.time()

max(lst_pr)

lst_pr.index(0.7789715454417516)

plt.figure(figsize = [12,10])
plt.scatter(lst_lr,lst_pr,color = 'darkblue')
plt.legend(['yolo'])

plt.style?